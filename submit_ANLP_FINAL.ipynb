{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "submit ANLP FINAL",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idBW9olEPYGR"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uNIu6ih72wP",
        "outputId": "6ee46a57-553c-494a-aeb6-e40a777dcb1d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TIEdUY523BF",
        "outputId": "5a2ec64b-5c05-4e99-f337-4a71b60dc735"
      },
      "source": [
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-20 15:25:43--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.97.181\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.97.181|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231508 (226K) [text/plain]\n",
            "Saving to: ‘bert-base-uncased-vocab.txt.1’\n",
            "\n",
            "bert-base-uncased-v 100%[===================>] 226.08K   917KB/s    in 0.2s    \n",
            "\n",
            "2021-03-20 15:25:43 (917 KB/s) - ‘bert-base-uncased-vocab.txt.1’ saved [231508/231508]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXmZ6ISegREN",
        "outputId": "97148b17-8eaf-4921-ce1a-4123555c4e6b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path=\"/content/drive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwRIaL1U7Ptp"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from transformers import TFBertModel\n",
        "from tokenizers import BertWordPieceTokenizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOtrxNVY7Ptq"
      },
      "source": [
        "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
        "\n",
        "train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
        "eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
        "\n",
        "train_path = keras.utils.get_file(\"train.json\", train_data_url)\n",
        "eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)\n",
        "\n",
        "max_len = 400\n",
        "\n",
        "with open(train_path) as f:\n",
        "    raw_train_data = json.load(f)\n",
        "\n",
        "with open(eval_path) as f:\n",
        "    raw_eval_data = json.load(f)\n",
        "\n",
        "train_all = False\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMrP0HK-OXpO"
      },
      "source": [
        "# **Preprocessing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqW9F17QHD5c"
      },
      "source": [
        "def convert_squad2data(raw_data):\n",
        "    squad_data = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                has_answer = True if qa['answers'] else False\n",
        "                if has_answer:\n",
        "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                    answer_start_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                    squad_sample = SquadDataSample(question, context, answer_start_idx, answer_text, all_answers)\n",
        "                else:\n",
        "                    squad_sample = SquadDataSample(question, context)\n",
        "                squad_sample.preprocess()\n",
        "                squad_data.append(squad_sample)\n",
        "    return squad_data\n",
        "\n",
        "\n",
        "def create_input_dataset(squad_samples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for sample in squad_samples:\n",
        "        if not sample.skip:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(sample, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGzwg_al7Pts"
      },
      "source": [
        "class SquadDataSample:\n",
        "    def __init__(self, question, context, answer_start_idx=0, answer_text=None, all_answers=None):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.answer_start_idx = answer_start_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        context = self.context\n",
        "        question = self.question\n",
        "        answer_text = self.answer_text\n",
        "        answer_start_idx = self.answer_start_idx\n",
        "\n",
        "        context = \" \".join(str(context).split())\n",
        "        question = \" \".join(str(question).split())\n",
        "\n",
        "        tokenized_context, tokenized_question = self.tokenize_data(context, question)\n",
        "\n",
        "        if self.answer_text is not None:\n",
        "            answer = \" \".join(str(answer_text).split())\n",
        "            answer_end_idx = answer_start_idx + len(answer)\n",
        "            if answer_end_idx >= len(context):\n",
        "                self.skip = True\n",
        "                return\n",
        "            is_char_in_ans = [0] * len(context)\n",
        "            for idx in range(answer_start_idx, answer_end_idx):\n",
        "                is_char_in_ans[idx] = 1\n",
        "            answer_token_idx = []\n",
        "            for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "                if sum(is_char_in_ans[start:end]) > 0:\n",
        "                    answer_token_idx.append(idx)\n",
        "            if len(answer_token_idx) == 0:\n",
        "                self.skip = True\n",
        "                return\n",
        "            start_token_idx = answer_token_idx[0]\n",
        "            end_token_idx = answer_token_idx[-1]\n",
        "        else:\n",
        "            start_token_idx = 0\n",
        "            end_token_idx = 0\n",
        "\n",
        "        attention_mask, input_ids, token_type_ids = self.create_sample_features(tokenized_context, tokenized_question)\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.context_token_to_char = tokenized_context.offsets\n",
        "\n",
        "    def create_sample_features(self, tokenized_context, tokenized_question):\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        if padding_length > 0:\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:\n",
        "            self.skip = True\n",
        "        return attention_mask, input_ids, token_type_ids\n",
        "\n",
        "    def tokenize_data(self, context, question):\n",
        "        tokenized_context = tokenizer.encode(context)\n",
        "        tokenized_question = tokenizer.encode(question)\n",
        "        return tokenized_context, tokenized_question\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUImCGaxh7NS",
        "outputId": "d4faf3d1-358a-43b4-bbe0-39d9808ad7d8"
      },
      "source": [
        "train_squad_samples = convert_squad2data(raw_train_data)\n",
        "eval_squad_samples = convert_squad2data(raw_eval_data)\n",
        "\n",
        "if train_all:\n",
        "    x_train, y_train = create_input_dataset(train_squad_samples)\n",
        "    x_eval, y_eval = create_input_dataset(eval_squad_samples)\n",
        "    x_test, y_test = x_eval, y_eval\n",
        "\n",
        "    print(\"num of train data: \" + str(len(train_squad_samples)))\n",
        "    print(\"num of eval data: \" + str(len(eval_squad_samples)))\n",
        "else:\n",
        "    n_train = 30000\n",
        "    n_eval = 5000\n",
        "    n_test = 600\n",
        "    x_train, y_train = create_input_dataset(train_squad_samples[:n_train])\n",
        "    x_eval, y_eval = create_input_dataset(eval_squad_samples[:n_eval])\n",
        "    x_test, y_test = create_input_dataset(eval_squad_samples[n_eval:n_eval + n_test])\n",
        "\n",
        "    print(\"num of train data used for train: \" + str(len(train_squad_samples[:n_train])))\n",
        "    print(\"num of eval data used for eval: \" + str(len(eval_squad_samples[:n_eval])))\n",
        "    print(\"num of train data used for test: \" + str(len(eval_squad_samples[n_eval:n_eval + n_test])))\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num of train data used for train: 30000\n",
            "num of eval data used for eval: 5000\n",
            "num of train data used for test: 600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7jGBLKNOkGm"
      },
      "source": [
        "# **Create model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEJAQAIb7Pt5"
      },
      "source": [
        "def create_model(learning_rate=5e-5, decay=1e-6):\n",
        "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    embedding = bert_model.bert(\n",
        "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        "    )[0]\n",
        "\n",
        "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
        "    end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[start_probs, end_probs],\n",
        "    )\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss], metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSSKo6jG7Pt8"
      },
      "source": [
        "# **Train and Evaluate model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPpE5JN6PBKA"
      },
      "source": [
        "Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7_W7Nfv7Pt6"
      },
      "source": [
        "# model = create_model(learning_rate=5e-5, decay=1e-6)\n",
        "# model.summary()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Pei6tN7Pt8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5d0486-52af-4d04-8648-24c91ec0986d"
      },
      "source": [
        "epochs = 3\n",
        "batch_size = 8\n",
        "model = create_model(learning_rate=3e-5, decay=1e-6)\n",
        "# model.summary()\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "      validation_data=(x_eval, y_eval))\n",
        "\n",
        "model.save(path+\"save/model.h5\")\n",
        "\n",
        "print(\"TRAINING PROCESS DONE!!!!!!!!!!!!!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "3025/3714 [=======================>......] - ETA: 15:21 - loss: 3.8113 - activation_2_loss: 1.9284 - activation_3_loss: 1.8829 - activation_2_accuracy: 0.4684 - activation_3_accuracy: 0.4755"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsnM7bPuPM29"
      },
      "source": [
        "Evaluate model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7Od3Yq4EzIE"
      },
      "source": [
        "model =keras.models.load_model(path+\"save/model.h5\")\n",
        "\n",
        "model.evaluate(x_test,y_test,batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl4OgmKDPFT0"
      },
      "source": [
        "Test model with custom data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4Rn_qG5Yu3s"
      },
      "source": [
        "test_data = {\"data\":\n",
        "    [\n",
        "        {\"title\": \"Project Apollo\",\n",
        "         \"paragraphs\": [\n",
        "             {\n",
        "                \"context\": \"The pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at 1 m\\u00b7s\\u22122 when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sth\\u00e8ne, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf.\",\n",
        "                 \"qas\": [\n",
        "                         {\"question\": \"What is the metric term less used than the Newton?\", \"id\": \"5737aafd1c456719005744fb\", \n",
        "                          \"answers\": [{\"text\": \"kilogram-force\", \"answer_start\": 82}, \n",
        "                                      {\"text\": \"pound-force\", \"answer_start\": 4}, \n",
        "                                      {\"text\": \"kilogram-force (kgf)\", \"answer_start\": 82}, \n",
        "                                      {\"text\": \"kilogram-force\", \"answer_start\": 82}, \n",
        "                                      {\"text\": \"the kilogram-force (\", \"answer_start\": 78}]}, \n",
        "                         {\"question\": \"What is the kilogram-force sometimes reffered to as?\", \"id\": \"5737aafd1c456719005744fc\", \n",
        "                          \"answers\": [{\"text\": \"kilopond\", \"answer_start\": 114}, {\"text\": \"kilopond\", \"answer_start\": 114}, {\"text\": \"kilopond\", \"answer_start\": 114}, {\"text\": \"kilopond\", \"answer_start\": 114}, {\"text\": \"kilopond\", \"answer_start\": 114}]}, \n",
        "                         {\"question\": \"What is a very seldom used unit of mass in the metric system?\", \"id\": \"5737aafd1c456719005744fd\", \n",
        "                          \"answers\": [{\"text\": \"slug\", \"answer_start\": 274}, {\"text\": \"metric slug\", \"answer_start\": 267}, {\"text\": \"metric slug\", \"answer_start\": 267}, {\"text\": \"metric slug\", \"answer_start\": 267}, {\"text\": \"the metric slug\", \"answer_start\": 263}]}, \n",
        "                         {\"question\": \"What seldom used term of a unit of force equal to 1000 pound s of force?\", \"id\": \"5737aafd1c456719005744fe\", \n",
        "                          \"answers\": [{\"text\": \"kip\", \"answer_start\": 712}, {\"text\": \"kip\", \"answer_start\": 712}, {\"text\": \"kip\", \"answer_start\": 712}, {\"text\": \"kip\", \"answer_start\": 712}, {\"text\": \"kip\", \"answer_start\": 712}]}, \n",
        "                         {\"question\": \"What is the seldom used force unit equal to one thousand newtons?\", \"id\": \"5737aafd1c456719005744ff\", \n",
        "                          \"answers\": [{\"text\": \"sth\\u00e8ne\", \"answer_start\": 665}, {\"text\": \"sth\\u00e8ne\", \"answer_start\": 665}, {\"text\": \"sth\\u00e8ne\", \"answer_start\": 665}, {\"text\": \"sth\\u00e8ne\", \"answer_start\": 665}, {\"text\": \"sth\\u00e8ne\", \"answer_start\": 665}]}, \n",
        "]}]}]}\n",
        "\n",
        "\n",
        "test_samples = convert_squad2data(test_data)\n",
        "x_test, _ = create_input_dataset(test_samples)\n",
        "\n",
        "pred_start, pred_end = model.predict(x_test)\n",
        "\n",
        "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "    test_sample = test_samples[idx]\n",
        "    offsets = test_sample.context_token_to_char\n",
        "    start = np.argmax(start)\n",
        "    end = np.argmax(end)\n",
        "    pred_ans = None\n",
        "    if start >= len(offsets):\n",
        "        continue\n",
        "    pred_char_start = offsets[start][0]\n",
        "    if end < len(offsets):\n",
        "        pred_char_end = offsets[end][1]\n",
        "        pred_ans = test_sample.context[pred_char_start:pred_char_end]\n",
        "    else:\n",
        "        pred_ans = test_sample.context[pred_char_start:]\n",
        "\n",
        "    print(\"Question: \" + test_sample.question)\n",
        "    print(\"Predict answer: \" + pred_ans)\n",
        "    if test_samples[idx].answer_text:\n",
        "        print(\"Correct answer: \" + test_samples[idx].answer_text)\n",
        "        for a in test_samples[idx].all_answers:\n",
        "            print(\"All possible answer: \" + a)\n",
        "    print(\"+++++++++++++++++++++++++++++++++++++++++\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPHM0Dh4MILx"
      },
      "source": [
        "# **Question-Answering Bot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHRfT6UUMbQi"
      },
      "source": [
        "!pip install discord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toLS2TgTMG8M"
      },
      "source": [
        "def convert_input2squad(context, question):\n",
        "    data = []\n",
        "    para = []\n",
        "    qas = []\n",
        "    qa = {\n",
        "        \"question\": question,\n",
        "        \"answers\": [],\n",
        "        \"id\": 0\n",
        "    }\n",
        "    qas.append(qa)\n",
        "    p = {\n",
        "        \"context\": context,\n",
        "        \"qas\": qas\n",
        "    }\n",
        "\n",
        "    para.append(p)\n",
        "    paras = {\n",
        "        \"paragraphs\": para\n",
        "    }\n",
        "    data.append(paras)\n",
        "\n",
        "    squad_data = {\n",
        "        \"data\": data\n",
        "    }\n",
        "    return squad_data\n",
        "\n",
        "\n",
        "def predict_answer(question, context):\n",
        "    data = convert_input2squad(context, question)\n",
        "    samples = convert_squad2data(data)\n",
        "    x_test, _ = create_input_dataset(samples)\n",
        "\n",
        "    pred_start, pred_end = model.predict(x_test)\n",
        "\n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "        sample = samples[idx]\n",
        "        offsets = sample.context_token_to_char\n",
        "        start = np.argmax(start)\n",
        "        end = np.argmax(end)\n",
        "        pred_answer = None\n",
        "        if start >= len(offsets):\n",
        "            continue\n",
        "        pred_char_start = offsets[start][0]\n",
        "        if end < len(offsets):\n",
        "            pred_char_end = offsets[end][1]\n",
        "            pred_answer = sample.context[pred_char_start:pred_char_end]\n",
        "        else:\n",
        "            pred_answer = sample.context[pred_char_start:]\n",
        "    return pred_answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw4U5aY6MYU0"
      },
      "source": [
        "import discord\n",
        "import nest_asyncio\n",
        "import time\n",
        "nest_asyncio.apply()\n",
        "\n",
        "class MyClient(discord.Client):\n",
        "    async def on_ready(self):\n",
        "        print('Logged in as')\n",
        "        print(self.user.name)\n",
        "        print(self.user.id)\n",
        "        print('------')\n",
        "\n",
        "    async def on_message(self, message):\n",
        "        if message.author.id == self.user.id:\n",
        "            return\n",
        "        else:\n",
        "            question = message.content\n",
        "            start = time.time()\n",
        "            answer = call_model(question)\n",
        "            print(\"answer: \", answer)\n",
        "\n",
        "            if answer:\n",
        "              await message.channel.send(answer.format(message))\n",
        "            else:\n",
        "              await message.channel.send(\"Cannot found the answer!\".format(message))\n",
        "              \n",
        "            end = time.time()\n",
        "            print(\"time: \",end - start)\n",
        "\n",
        "\n",
        "client = MyClient()\n",
        "client.run('ODE4Nzk2NDAzMDEwMTc0OTc3.YEdRkg.aDIJhTI6aNL6HaUk0RADiI_J-DM')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}